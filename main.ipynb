{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_extraction import DataExtractor\n",
    "from database_utils import DatabaseConnector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine(postgresql://aicore_admin:***@data-handling-project-readonly.cq2e8zno855e.eu-west-1.rds.amazonaws.com:5432/postgres)\n",
      "['legacy_store_details', 'legacy_users', 'orders_table']\n"
     ]
    }
   ],
   "source": [
    "# Initialize DatabaseConnector\n",
    "db_connector = DatabaseConnector()\n",
    "\n",
    "#read database credentials from YAML file and initialize database engine\n",
    "cred_data = db_connector.read_db_creds()\n",
    "engine = db_connector.init_db_engine(cred_data)\n",
    "print(engine)\n",
    "    \n",
    "#initialize DataExtractor \n",
    "extractor = DataExtractor(engine)\n",
    "\n",
    "#get the names of all tables in the database\n",
    "tables = db_connector.list_db_tables(engine)\n",
    "print(tables)\n",
    "\n",
    "#loop through each table and extract data into a DataFrame\n",
    "for table_name in tables:\n",
    "    df = extractor.read_rds_table(table_name)\n",
    "    df.to_csv(table_name+'.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data\n",
    "from data_cleaning import DataCleaning\n",
    "from data_upload import DataLoader\n",
    "import pandas as pd \n",
    "\n",
    "data_cleaner = DataCleaning()\n",
    "df = pd.read_csv('legacy_users.csv')\n",
    "df = data_cleaner.clean_user_data(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df.drop(df.columns[0:2], axis=1, inplace=True)\n",
    "df.to_csv('dim_users.csv')\n",
    "\n",
    "# Uploading to postgres database \n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_local_creds()\n",
    "local_engine = db_connector.local_db_engine(cred_data)\n",
    "\n",
    "data_loader = DataLoader(local_engine)\n",
    "data_loader.upload_to_db(df,'dim_users') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error importing jpype dependencies. Fallback to subprocess.\n",
      "No module named 'jpype'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "            card_number expiry_date                card_provider  \\\n",
      "0        30060773296197       09/26  Diners Club / Carte Blanche   \n",
      "1       349624180933183       10/23             American Express   \n",
      "2      3529023891650490       06/23                 JCB 16 digit   \n",
      "3       213142929492281       09/27                 JCB 15 digit   \n",
      "4          502067329974       10/25                      Maestro   \n",
      "...                 ...         ...                          ...   \n",
      "15304   180036921556789       12/28                 JCB 15 digit   \n",
      "15305   180018030448512       11/24                 JCB 15 digit   \n",
      "15306  3569953313547220       04/24                 JCB 16 digit   \n",
      "15307  4444521712606810       06/27                VISA 16 digit   \n",
      "15308   372031786522735       02/30             American Express   \n",
      "\n",
      "      date_payment_confirmed  \n",
      "0                 2015-11-25  \n",
      "1                 2001-06-18  \n",
      "2                 2000-12-26  \n",
      "3                 2011-02-12  \n",
      "4                 1997-03-13  \n",
      "...                      ...  \n",
      "15304             1997-06-06  \n",
      "15305             2004-06-16  \n",
      "15306             2020-02-05  \n",
      "15307             2008-06-16  \n",
      "15308             2009-02-04  \n",
      "\n",
      "[15309 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "link = \"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\"\n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_db_creds()\n",
    "engine = db_connector.init_db_engine(cred_data)\n",
    "pdf_extractor = DataExtractor(engine)\n",
    "df = pdf_extractor.retrieve_pdf_data(link)\n",
    "print(type(df))\n",
    "print(df)\n",
    "df.to_csv('card_details.csv',index= False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import DataCleaning\n",
    "from data_upload import DataLoader\n",
    "\n",
    "\n",
    "data_cleaner = DataCleaning()\n",
    "df = pd.read_csv('card_details.csv')\n",
    "df = data_cleaner.clean_card_data(df)\n",
    "df.to_csv('dim_card_details.csv')\n",
    "\n",
    "# Uploading to postgres database \n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_local_creds()\n",
    "local_engine = db_connector.local_db_engine(cred_data)\n",
    "\n",
    "data_loader = DataLoader(local_engine)\n",
    "data_loader.upload_to_db(df,'dim_card_details') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: The store data is retrieved through the use of an API.\n",
    "\n",
    "The API has two GET methods. One will return the number of stores in the business and the other to retrieve a store given a store number.\n",
    "\n",
    "A dictionary stores the header details it will have a key x-api-key with the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "{'statusCode': 200, 'number_stores': 451}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'index': 2,\n",
       " 'address': 'Heckerstraße 4/5\\n50491 Säckingen, Landshut',\n",
       " 'longitude': '48.52961',\n",
       " 'lat': None,\n",
       " 'locality': 'Landshut',\n",
       " 'store_code': 'LA-0772C7B9',\n",
       " 'staff_numbers': '92',\n",
       " 'opening_date': '2013-04-12',\n",
       " 'store_type': 'Super Store',\n",
       " 'latitude': '12.16179',\n",
       " 'country_code': 'DE',\n",
       " 'continent': 'Europe'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The store data can be retrieved through the use of an API.\n",
    "from data_extraction import DataExtractor\n",
    "\n",
    "header_dict = {'x-api-key' : 'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'}\n",
    "store_retrieve = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/2'\n",
    "storenum_endp= 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores'\n",
    "\n",
    "data_extractor = DataExtractor(engine) # engine being problematic?!?!?!?!?!\n",
    "data_extractor.list_number_of_stores(storenum_endp,header_dict)\n",
    "data_extractor.retrieve_stores_data(store_retrieve,header_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5.3: Outputs the API extracted data to a DataFrame then CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range (0,451):\n",
    "    store_data = data_extractor.retrieve_stores_data(f'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/{i}',header_dict)\n",
    "    rows.append(store_data)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv('stores_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 423 entries, 0 to 422\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   address        423 non-null    object        \n",
      " 1   longitude      423 non-null    float64       \n",
      " 2   locality       423 non-null    object        \n",
      " 3   store_code     423 non-null    object        \n",
      " 4   staff_numbers  423 non-null    float64       \n",
      " 5   opening_date   423 non-null    datetime64[ns]\n",
      " 6   store_type     423 non-null    object        \n",
      " 7   latitude       423 non-null    float64       \n",
      " 8   country_code   423 non-null    object        \n",
      " 9   continent      423 non-null    object        \n",
      "dtypes: datetime64[ns](1), float64(3), object(6)\n",
      "memory usage: 33.2+ KB\n"
     ]
    }
   ],
   "source": [
    "from data_cleaning import DataCleaning\n",
    "from database_utils import DatabaseConnector\n",
    "from data_upload import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_cleaner = DataCleaning()\n",
    "df = pd.read_csv('stores_data.csv')\n",
    "df = data_cleaner.clean_store_data(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df.drop(df.columns[0:2], axis=1, inplace=True)\n",
    "df.to_csv('dim_store_details.csv')\n",
    "df.info()\n",
    "# Uploading to postgres database \n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_local_creds()\n",
    "local_engine = db_connector.local_db_engine(cred_data)\n",
    "\n",
    "data_loader = DataLoader(local_engine)\n",
    "data_loader.upload_to_db(df,'dim_store_details') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing product details from S3 bucket and cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import DataCleaning\n",
    "from data_extraction import DataExtractor\n",
    "from database_utils import DatabaseConnector\n",
    "\n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_db_creds()\n",
    "engine = db_connector.init_db_engine(cred_data)\n",
    "\n",
    "\n",
    "s3_extractor = DataExtractor(engine)\n",
    "bucket_name = 'data-handling-public'\n",
    "object_key = 'products.csv'\n",
    "df = s3_extractor.extract_from_s3(bucket_name,object_key)\n",
    "df.to_csv('products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1844 entries, 0 to 1843\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   product_name   1844 non-null   object        \n",
      " 1   product_price  1844 non-null   object        \n",
      " 2   weight         1844 non-null   object        \n",
      " 3   category       1844 non-null   object        \n",
      " 4   EAN            1844 non-null   object        \n",
      " 5   date_added     1844 non-null   datetime64[ns]\n",
      " 6   uuid           1844 non-null   object        \n",
      " 7   removed        1844 non-null   object        \n",
      " 8   product_code   1844 non-null   object        \n",
      "dtypes: datetime64[ns](1), object(8)\n",
      "memory usage: 129.8+ KB\n"
     ]
    }
   ],
   "source": [
    "from data_cleaning import DataCleaning\n",
    "from database_utils import DatabaseConnector\n",
    "from data_upload import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "data_cleaner = DataCleaning()\n",
    "df = pd.read_csv('products.csv')\n",
    "df = data_cleaner.clean_product_data(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df.drop(df.columns[0:2], axis=1, inplace=True)\n",
    "df.to_csv('dim_products.csv')\n",
    "df.info()\n",
    "# Uploading to postgres database \n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_local_creds()\n",
    "local_engine = db_connector.local_db_engine(cred_data)\n",
    "\n",
    "data_loader = DataLoader(local_engine)\n",
    "data_loader.upload_to_db(df,'dim_products') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['toys-and-games', 'sports-and-leisure', 'pets', 'homeware',\n",
       "       'health-and-beauty', 'food-and-drink', 'diy'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_cleaning import DataCleaning\n",
    "import pandas as pd\n",
    "\n",
    "data_cleaner = DataCleaning()\n",
    "df = pd.read_csv('orders_table.csv')\n",
    "df = data_cleaner.clean_orders_data(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df.drop(df.columns[0:3], axis=1, inplace=True)\n",
    "df.to_csv('clean_orders.csv')\n",
    "\n",
    "# Uploading to postgres database \n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_local_creds()\n",
    "local_engine = db_connector.local_db_engine(cred_data)\n",
    "\n",
    "data_loader = DataLoader(local_engine)\n",
    "data_loader.upload_to_db(df,'orders_table') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_extraction import DataExtractor\n",
    "from database_utils import DatabaseConnector\n",
    "\n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_db_creds()\n",
    "engine = db_connector.init_db_engine(cred_data)\n",
    "\n",
    "s3_extractor = DataExtractor(engine)\n",
    "bucket_name = 'data-handling-public'\n",
    "object_key = 'date_details.json'\n",
    "df = s3_extractor.extract_from_s3(bucket_name,object_key)\n",
    "df.to_csv('date_details.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m cred_data \u001b[38;5;241m=\u001b[39m db_connector\u001b[38;5;241m.\u001b[39mread_local_creds()\n\u001b[1;32m     16\u001b[0m local_engine \u001b[38;5;241m=\u001b[39m db_connector\u001b[38;5;241m.\u001b[39mlocal_db_engine(cred_data)\n\u001b[0;32m---> 18\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(local_engine)\n\u001b[1;32m     19\u001b[0m data_loader\u001b[38;5;241m.\u001b[39mupload_to_db(df,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_date_times\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_cleaning import DataCleaning\n",
    "from data_upload import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "data_cleaner = DataCleaning()\n",
    "df = pd.read_csv('date_details.csv')\n",
    "df = data_cleaner.clean_date_details(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df.drop(df.columns[0:3], axis=1, inplace=True)\n",
    "df.drop(columns=['year','day'], axis=1, inplace=True)\n",
    "df.to_csv('dim_date_times.csv')\n",
    "\n",
    "# Uploading to postgres database \n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_local_creds()\n",
    "local_engine = db_connector.local_db_engine(cred_data)\n",
    "\n",
    "data_loader = DataLoader(local_engine)\n",
    "data_loader.upload_to_db(df,'dim_date_times') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
