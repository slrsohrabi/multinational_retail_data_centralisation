{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_extraction import DataExtractor\n",
    "from database_utils import DatabaseConnector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine(postgresql://aicore_admin:***@data-handling-project-readonly.cq2e8zno855e.eu-west-1.rds.amazonaws.com:5432/postgres)\n",
      "['legacy_store_details', 'legacy_users', 'orders_table']\n"
     ]
    }
   ],
   "source": [
    "# Initialize DatabaseConnector\n",
    "db_connector = DatabaseConnector()\n",
    "\n",
    "#read database credentials from YAML file and initialize database engine\n",
    "cred_data = db_connector.read_db_creds()\n",
    "engine = db_connector.init_db_engine(cred_data)\n",
    "print(engine)\n",
    "    \n",
    "#initialize DataExtractor \n",
    "extractor = DataExtractor(engine)\n",
    "\n",
    "#get the names of all tables in the database\n",
    "tables = db_connector.list_db_tables(engine)\n",
    "print(tables)\n",
    "\n",
    "#loop through each table and extract data into a DataFrame\n",
    "for table_name in tables:\n",
    "    df = extractor.read_rds_table(table_name)\n",
    "    df.to_csv(table_name+'.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data\n",
    "from data_cleaning import DataCleaning\n",
    "from data_upload import DataLoader\n",
    "import pandas as pd \n",
    "\n",
    "data_cleaner = DataCleaning()\n",
    "df = pd.read_csv('legacy_users.csv')\n",
    "df = data_cleaner.clean_user_data(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df.drop(df.columns[0:2], axis=1, inplace=True)\n",
    "df.to_csv('dim_users.csv')\n",
    "\n",
    "# Uploading to postgres database \n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_local_creds()\n",
    "local_engine = db_connector.local_db_engine(cred_data)\n",
    "\n",
    "data_loader = DataLoader(local_engine)\n",
    "data_loader.upload_to_db(df,'dim_users') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error importing jpype dependencies. Fallback to subprocess.\n",
      "No module named 'jpype'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "            card_number expiry_date                card_provider  \\\n",
      "0        30060773296197       09/26  Diners Club / Carte Blanche   \n",
      "1       349624180933183       10/23             American Express   \n",
      "2      3529023891650490       06/23                 JCB 16 digit   \n",
      "3       213142929492281       09/27                 JCB 15 digit   \n",
      "4          502067329974       10/25                      Maestro   \n",
      "...                 ...         ...                          ...   \n",
      "15304   180036921556789       12/28                 JCB 15 digit   \n",
      "15305   180018030448512       11/24                 JCB 15 digit   \n",
      "15306  3569953313547220       04/24                 JCB 16 digit   \n",
      "15307  4444521712606810       06/27                VISA 16 digit   \n",
      "15308   372031786522735       02/30             American Express   \n",
      "\n",
      "      date_payment_confirmed  \n",
      "0                 2015-11-25  \n",
      "1                 2001-06-18  \n",
      "2                 2000-12-26  \n",
      "3                 2011-02-12  \n",
      "4                 1997-03-13  \n",
      "...                      ...  \n",
      "15304             1997-06-06  \n",
      "15305             2004-06-16  \n",
      "15306             2020-02-05  \n",
      "15307             2008-06-16  \n",
      "15308             2009-02-04  \n",
      "\n",
      "[15309 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "link = \"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\"\n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_db_creds()\n",
    "engine = db_connector.init_db_engine(cred_data)\n",
    "pdf_extractor = DataExtractor(engine)\n",
    "df = pdf_extractor.retrieve_pdf_data(link)\n",
    "print(type(df))\n",
    "print(df)\n",
    "df.to_csv('card_details.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import DataCleaning\n",
    "from data_upload import DataLoader\n",
    "\n",
    "\n",
    "data_cleaner = DataCleaning()\n",
    "df = pd.read_csv('card_details.csv')\n",
    "df = data_cleaner.clean_card_data(df)\n",
    "df.to_csv('dim_card_details.csv')\n",
    "\n",
    "# Uploading to postgres database \n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_local_creds()\n",
    "local_engine = db_connector.local_db_engine(cred_data)\n",
    "\n",
    "data_loader = DataLoader(local_engine)\n",
    "data_loader.upload_to_db(df,'dim_card_details') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The store data is retrieved through the use of an API.\n",
    "\n",
    "The API has two GET methods. One will return the number of stores in the business and the other to retrieve a tore given a store number.\n",
    "\n",
    "A dictionary stores the header details it will have a key x-api-key with the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "{'statusCode': 200, 'number_stores': 451}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'index': 2,\n",
       " 'address': 'Heckerstraße 4/5\\n50491 Säckingen, Landshut',\n",
       " 'longitude': '48.52961',\n",
       " 'lat': None,\n",
       " 'locality': 'Landshut',\n",
       " 'store_code': 'LA-0772C7B9',\n",
       " 'staff_numbers': '92',\n",
       " 'opening_date': '2013-04-12',\n",
       " 'store_type': 'Super Store',\n",
       " 'latitude': '12.16179',\n",
       " 'country_code': 'DE',\n",
       " 'continent': 'Europe'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The store data can be retrieved through the use of an API.\n",
    "from data_extraction import DataExtractor\n",
    "\n",
    "header_dict = {'x-api-key' : 'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'}\n",
    "store_retrieve = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/2'\n",
    "storenum_endp= 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores'\n",
    "\n",
    "data_extractor = DataExtractor(engine) # engine being problematic?!?!?!?!?!\n",
    "data_extractor.list_number_of_stores(storenum_endp,header_dict)\n",
    "data_extractor.retrieve_stores_data(store_retrieve,header_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs the API extracted data to a DataFrame then CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range (0,451):\n",
    "    store_data = data_extractor.retrieve_stores_data(f'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/{i}',header_dict)\n",
    "    rows.append(store_data)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv('stores_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 423 entries, 0 to 422\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   address        423 non-null    object        \n",
      " 1   longitude      423 non-null    float64       \n",
      " 2   locality       423 non-null    object        \n",
      " 3   store_code     423 non-null    object        \n",
      " 4   staff_numbers  423 non-null    float64       \n",
      " 5   opening_date   423 non-null    datetime64[ns]\n",
      " 6   store_type     423 non-null    object        \n",
      " 7   latitude       423 non-null    float64       \n",
      " 8   country_code   423 non-null    object        \n",
      " 9   continent      423 non-null    object        \n",
      "dtypes: datetime64[ns](1), float64(3), object(6)\n",
      "memory usage: 33.2+ KB\n"
     ]
    }
   ],
   "source": [
    "from data_cleaning import DataCleaning\n",
    "from database_utils import DatabaseConnector\n",
    "from data_upload import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_cleaner = DataCleaning()\n",
    "df = pd.read_csv('stores_data.csv')\n",
    "df = data_cleaner.clean_store_data(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df.drop(df.columns[0:2], axis=1, inplace=True)\n",
    "df.to_csv('dim_store_details.csv')\n",
    "df.info()\n",
    "# Uploading to postgres database \n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_local_creds()\n",
    "local_engine = db_connector.local_db_engine(cred_data)\n",
    "\n",
    "data_loader = DataLoader(local_engine)\n",
    "data_loader.upload_to_db(df,'dim_store_details') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing product details from S3 bucket and cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import DataCleaning\n",
    "from data_extraction import DataExtractor\n",
    "from database_utils import DatabaseConnector\n",
    "\n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_db_creds()\n",
    "engine = db_connector.init_db_engine(cred_data)\n",
    "\n",
    "\n",
    "s3_extractor = DataExtractor(engine)\n",
    "bucket_name = 'data-handling-public'\n",
    "object_key = 'products.csv'\n",
    "df = s3_extractor.extract_from_s3(bucket_name,object_key)\n",
    "df.to_csv('products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1844 entries, 0 to 1843\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   product_name   1844 non-null   object        \n",
      " 1   product_price  1844 non-null   object        \n",
      " 2   weight         1844 non-null   object        \n",
      " 3   category       1844 non-null   object        \n",
      " 4   EAN            1844 non-null   object        \n",
      " 5   date_added     1844 non-null   datetime64[ns]\n",
      " 6   uuid           1844 non-null   object        \n",
      " 7   removed        1844 non-null   object        \n",
      " 8   product_code   1844 non-null   object        \n",
      "dtypes: datetime64[ns](1), object(8)\n",
      "memory usage: 129.8+ KB\n"
     ]
    }
   ],
   "source": [
    "from data_cleaning import DataCleaning\n",
    "from database_utils import DatabaseConnector\n",
    "from data_upload import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "data_cleaner = DataCleaning()\n",
    "df = pd.read_csv('products.csv')\n",
    "df = data_cleaner.clean_product_data(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df.drop(df.columns[0:2], axis=1, inplace=True)\n",
    "df.to_csv('dim_products.csv')\n",
    "df.info()\n",
    "# Uploading to postgres database \n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_local_creds()\n",
    "local_engine = db_connector.local_db_engine(cred_data)\n",
    "\n",
    "data_loader = DataLoader(local_engine)\n",
    "data_loader.upload_to_db(df,'dim_products') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['toys-and-games', 'sports-and-leisure', 'pets', 'homeware',\n",
       "       'health-and-beauty', 'food-and-drink', 'diy'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_cleaning import DataCleaning\n",
    "import pandas as pd\n",
    "\n",
    "data_cleaner = DataCleaning()\n",
    "df = pd.read_csv('orders_table.csv')\n",
    "df = data_cleaner.clean_orders_data(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df.drop(df.columns[0:3], axis=1, inplace=True)\n",
    "df.to_csv('clean_orders.csv')\n",
    "\n",
    "# Uploading to postgres database \n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_local_creds()\n",
    "local_engine = db_connector.local_db_engine(cred_data)\n",
    "\n",
    "data_loader = DataLoader(local_engine)\n",
    "data_loader.upload_to_db(df,'orders_table') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_extraction import DataExtractor\n",
    "from database_utils import DatabaseConnector\n",
    "\n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_db_creds()\n",
    "engine = db_connector.init_db_engine(cred_data)\n",
    "\n",
    "s3_extractor = DataExtractor(engine)\n",
    "bucket_name = 'data-handling-public'\n",
    "object_key = 'date_details.json'\n",
    "df = s3_extractor.extract_from_s3(bucket_name,object_key)\n",
    "df.to_csv('date_details.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_cleaning import DataCleaning\n",
    "from data_upload import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "data_cleaner = DataCleaning()\n",
    "df = pd.read_csv('date_details.csv')\n",
    "df = data_cleaner.clean_date_details(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df.drop(df.columns[0:3], axis=1, inplace=True)\n",
    "df.drop(columns=['year','day'], axis=1, inplace=True)\n",
    "df.to_csv('dim_date_times.csv')\n",
    "\n",
    "# Uploading to postgres database \n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_local_creds()\n",
    "local_engine = db_connector.local_db_engine(cred_data)\n",
    "\n",
    "data_loader = DataLoader(local_engine)\n",
    "data_loader.upload_to_db(df,'dim_date_times') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Querry to get the information on columnd data types:\n",
    "SELECT column_name, data_type\n",
    "   FROM information_schema.columns\n",
    "   WHERE table_name = 'orders_table';\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" To change the column type to UUID(Universally Unique Identifier):\n",
    "ALTER TABLE orders_table\n",
    "ALTER COLUMN date_uuid TYPE UUID USING date_uuid::UUID;\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Another example of change made\n",
    "ALTER TABLE dim_users\n",
    "ALTER COLUMN country_code TYPE VARCHAR(2) USING country_code::VARCHAR(2);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine(postgresql://postgres:***@localhost:5434/sales_data)\n"
     ]
    },
    {
     "ename": "ObjectNotExecutableError",
     "evalue": "Not an executable object: 'ALTER TABLE orders_table ALTER COLUMN longitude TYPE FLOAT USING longitude::FLOAT;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/mrdc_env/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1412\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[43mstatement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_on_connection\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '_execute_on_connection'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mObjectNotExecutableError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m cleaner \u001b[38;5;241m=\u001b[39m DataCleaning()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m table_names:\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mcleaner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_datatype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_engine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AiCore/multinational_retail_data_centralisation/data_cleaning.py:173\u001b[0m, in \u001b[0;36mDataCleaning.set_datatype\u001b[0;34m(self, engine, table, df)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;66;03m# ALTER TABLE query for each row\u001b[39;00m\n\u001b[1;32m    172\u001b[0m         query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALTER TABLE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ALTER COLUMN \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_details_table\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m TYPE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequired_date_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m USING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_details_table\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m::\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequired_date_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 173\u001b[0m         q_exec \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery successful\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, columns\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/mrdc_env/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1414\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1412\u001b[0m     meth \u001b[38;5;241m=\u001b[39m statement\u001b[38;5;241m.\u001b[39m_execute_on_connection\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 1414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meth(\n\u001b[1;32m   1417\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1418\u001b[0m         distilled_parameters,\n\u001b[1;32m   1419\u001b[0m         execution_options \u001b[38;5;129;01mor\u001b[39;00m NO_OPTIONS,\n\u001b[1;32m   1420\u001b[0m     )\n",
      "\u001b[0;31mObjectNotExecutableError\u001b[0m: Not an executable object: 'ALTER TABLE orders_table ALTER COLUMN longitude TYPE FLOAT USING longitude::FLOAT;'"
     ]
    }
   ],
   "source": [
    "\"\"\" Decided to automate the data type cleaning procedure, required data_types stored t\"\"\"\n",
    "import pandas as pd\n",
    "from data_cleaning import DataCleaning\n",
    "from database_utils import DatabaseConnector\n",
    "\n",
    "table_names = ['orders_table','dim_users','dim_store_details']\n",
    "dtype_df = pd.read_csv('data_types.csv')\n",
    "\n",
    "db_connector = DatabaseConnector()\n",
    "cred_data = db_connector.read_local_creds()\n",
    "local_engine = db_connector.local_db_engine(cred_data)\n",
    "print(local_engine)\n",
    "cleaner = DataCleaning()\n",
    "for table in table_names:\n",
    "    cleaner.set_datatype(local_engine, table, dtype_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
